{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1633189663748)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel , Pipeline}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier,\n",
    "                                          RandomForestClassifier, RandomForestClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.\n",
    "    option(\"inferSchema\",true).\n",
    "    option(\"header\",false).\n",
    "    csv(\"Data/covtype/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithoutHeader.first\n",
    "//디시전 트리를 사용하려면 헤더 이름 즉 피쳐 이름이 있어야 한다.\n",
    "//그렇기 떄문에 아래쪽에서 피쳐 이름을 넣어주도록 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataWithoutHeader.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colNames = Seq(\n",
    "    \"Elevation\",\"Aspect\",\"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\") ++\n",
    "    ((0 until 4).map(i => s\"Wilderness_Area_$i\")) ++\n",
    "    ((0 until 40).map(i => s\"Soil_Type_$i\")) ++ Seq(\"Cover_Type\")\n",
    "\n",
    "val data = dataWithoutHeader.toDF(colNames:_*).withColumn(\"Cover_Type\",$\"Cover_Type\".cast(\"double\"))\n",
    "\n",
    "data.printSchema()\n",
    "\n",
    "//칼럼 이름을 넣어주기 윟서 이전에 본 피쳐에 대한 정보가 있는 인포메이션 파일을 보고 \n",
    "//그에 맞추어서 다 이름을 넣어주면 된다. 위 코드에서는 칼럼의 시퀀스로 만들어서 안에 넣어주었다.\n",
    "//스키마를 학인하면 헤더가 잘 들어가 있는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainData, testData) = data.randomSplit(Array(0.9 , 0.1))\n",
    "//tr데이터와 test 데이터를 0.9와 0.1로 나누었다.\n",
    "trainData.cache()\n",
    "testData.cache()\n",
    "//계속 볼거니까 캐시에 올렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "//스파크에서 ml로 디시전 트리를 돌려보기 위해서는 모든 인풋 피쳐 칼럼이 하나의 칼럼으로\n",
    "//구성되어있다. 이걸 위해 벡터어셈블러를 이용할 것이다.\n",
    "//tr데이터 칼럼 중에 프리딕션 해야하는 아웃풋 타입만 제외하고 나머지는 다 모았다.\n",
    "//그 모든 것을 가지고 벡터 어셈블러를 만들 것이다. 그리고 그걸 피쳐벡터라는 이름으로 만들겠다는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select(\"featureVector\").show(truncate = false)\n",
    "\n",
    "//tr데이터에 대해서 실제 트랜스폼을 돌려서 어셈블된 tr데이터를 만들 것이다. 여기서 피쳐벡터를 보여줘 하면\n",
    "//하나로 만들어진 피쳐벡터를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeatureCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "//랜덤 시드를 세틸한 후 예측할 레이블 칼럼이 뭐냐 해서 안에 넣어주고 이후 그럼 사용할 \n",
    "//피쳐 벡터는 뭐냐 해서 피쳐벡터 넣어주고 그럼 예측한 내용을 어디다 할거냐 해서 \n",
    "//이름을 프리딕션으로 지은 칼럼에 넣어주기로 한 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "50: error: not found: value classifier",
     "output_type": "error",
     "traceback": [
      "<console>:50: error: not found: value classifier",
      "       val model = classifier.fit(assembledTrainData)",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "val model = classifier.fit(assembledTrainData)\n",
    "println(model.toDebugString)\n",
    "\n",
    "//코어개수가 많이 없으면 커널이 죽을 수도 있다. 투디버그스트링을 하면 학습 이후의 만들어진 디시전 트리를 눈으로\n",
    "//확인 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)\n",
    "\n",
    "//어떤 피쳐가 중요한지 확인할 수 있다. 즉 엔트로피가 낮은 피쳐들이 들어갈 것 이다.\n",
    "//설명가능한 머신러닝 모델이라는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(assembledTrainData)\n",
    "\n",
    "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").\n",
    "    show(truncate = false)\n",
    "\n",
    "//커버타입 , 프리딕션 , 그게 얼마나 맞는지에 대한 확률을 볼 수 있다.\n",
    "//결과가 맘에 들지 않을 수도 있다. 우리가 하이퍼파라미터 튜닝을 하지 않았기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "\n",
    "//모델을 이벨류에이션 하기위해 이벨류에이터를 만든다.\n",
    "//이후 커버타입과 프리딕션을 비교하고 애큐러시를 만들고 정확도를 기반으로 이벨류에이션\n",
    "//하겠다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictionRDD = predictions.\n",
    " select(\"prediction\",\"Cover_Type\").\n",
    " as[(Double,Double)].rdd\n",
    "\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n",
    "println(multiclassMetrics.confusionMatrix)\n",
    "\n",
    "//모든 타겟 밸류에 대해 로우와 칼럼으로 되어있는 7 바이 7 메트릭스를 보자.\n",
    "//각각의 로우는 actual correct value / 각각의 칼럼은 predicted value 이다.\n",
    "//즉 정확한 판단 결과라면 대각선에만 값이 있어야 한다.\n",
    "//클래스 1번을 2번으로 판별하거나 이런 잘못 오인한 클래스파이어가 없어야 좋은 것이다.\n",
    "//즉 대각선에 값들이 모여있다면 좋은 프리딕션 일 것이고 아니면 나쁜 프리딕션 일 것이다.\n",
    "//RDD를 통해서 바꾸어 주고 , 그걸 가지고 다시 컨퓨전메트릭스를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//디시전 트리의 하이퍼파라미터의 튜닝을 한다.\n",
    "\n",
    "//뎁스라는 것인데 디시전 트리의 뎁스를 리밋을 걸어주는 것이다.\n",
    "//우리는 최대한 심플한 트리를 만들고 싶기 때문에 걸어줄 수 있따.\n",
    "\n",
    "//빈스는 디시전 룰의 개수를 의미한다. 디시전 룰의 개수가 최대한 작은 것이 좋아서\n",
    "//맥시멈 값을 정할 수 있다.\n",
    "\n",
    "//Impurity는 엔트로피와 같은 뜻이다.\n",
    "\n",
    "//미니멈 인포메이션 게인 즉 , 인포메이션 게인을 가장 큰걸 선택하려 하는데 \n",
    "//이걸 설정하면 이것보단 큰 걸 택해야 한다는 옵션이고 이걸 다 못넘으면 우리는\n",
    "//이후를 정할 수 없다는 뜻이다.\n",
    "\n",
    "//여기 나온 것들은 모두 디시전 트리의 컴플렉시티를 조정하겠다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val classifierHyper = new DecisionTreeClassifier().\n",
    "setSeed(Random.nextLong()).\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setFeatureCol(\"featureVector\").\n",
    "setPredictionCol(\"prediction\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, classifierHyper))\n",
    "\n",
    "//파이프라인을 만들어야 하는데 벡터 어셈블러가 인풋 피쳐를 만드는 것이어서 그게 있어야 하고\n",
    "//디시전 트리 클래스파이어가 있어야 한다.\n",
    "\n",
    "//우리가 루프를 돌면서 할때 파이프라인이 어셈블러만들어서 클래스파이어 만들고 이런 과정을\n",
    "//묶는 다는 개념이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//가장 좋은 하이퍼파라미터의 조합을 찾기위해 ParamGridBuilder를 사용한다.\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(classifierHyper.impurity, Seq(\"gini\",\"entropy\")).\n",
    "addGrid(classifierHyper.maxDepth,Seq(1,5)).\n",
    "addGrid(classifierHyper.maxBins, Seq(40,80)).\n",
    "addGrid(classifierHyper.minInfoGain,Seq(0,0,0.05)).\n",
    "build()\n",
    "\n",
    "//위의 코드처럼 세팅을 해놓아야 한다.\n",
    "//지니계수인지 엔트로피 인지 , 뎁스는 1인지 5로 할지 이런식으로 2개씩 4종류의 하이퍼 파라미터를\n",
    "//세팅했다. 즉 16개의 모델을 학습하고 베스트 파라미터 조합을 찾겠다는 마인드 이다.\n",
    "//메모리가 부족하거나 너무 느리면 코어를 늘리거나 메모리를 늘리는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//벨리데이션 모델을 만들고 파이프 라인을 만들게 되는데 모델을 이벨류에이션 하고 하이퍼\n",
    "//파라미터 서칭까지 하는 것을 파이프라인으로 만들게 된다.\n",
    "\n",
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "setLabelCol(\"Cober_Type\").\n",
    "setPredictionCol(\"prediction\").\n",
    "setMetricName(\"accuracy\")\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "setSeed(Random.nextLong()).\n",
    "setEstimator(pipeline).\n",
    "setEvaluator(multiclassEval).\n",
    "setEstimatorParamMaps(paramGrid).\n",
    "setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(trainData)\n",
    "\n",
    "//90프로를 tr에 쓰고 나머지를 벨리데이션에 쓰겠다는 것을 라티오 비율로 두었다.\n",
    "//커널이 죽거나 시간이 오래 걸리면 하이퍼 파라미터 개수를 줄이거나 메모리 늘리거나\n",
    "//코어 늘리거나 등으로 해결해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bestModel = vaildatorModel.bestModel\n",
    "bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap\n",
    "\n",
    "//위의 코드를 통해서 베스트 모델을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramAndMetrics = validatorModel.validationMetrics.\n",
    "zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "paramsAndMetrics.foreach{ case (metric, params) =>\n",
    "println(metric)\n",
    "println(params)\n",
    "println()\n",
    "}\n",
    "//베스트 모댈과 하이퍼 파라미터 조합에 대한 정확도도 다음과 같이 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validatorModel.validationMetrics.max\n",
    "multiclassEval.evaluate(bestMode.transform(testData))\n",
    "\n",
    "//이후 구한 베스트 모델에 대해서 test데이터를 넣어서 결과를 확인해보면\n",
    "//앞에서 구했던 하이퍼 파라미터 없이 만든 디시전 트리보다는\n",
    "//좋은 결과가 나올 것이라는 것을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
