{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.conf.Configuration\n",
       "import org.apache.hadoop.io.{LongWritable, Text}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import scala.collection.JavaConverters._\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import java.util.Properties\n",
       "import java.io.ByteArrayInputStream\n",
       "import edu.umd.cloud9.collection.XMLInputFormat\n",
       "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
       "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
       "import info.bliki.wiki.dump._\n",
       "import org.xml.sax.SAXException\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//외부 라이브러리 필요\n",
    "//자연어 처리 예시 LSA\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer , IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import java.util.Properties\n",
    "import java.io.ByteArrayInputStream;\n",
    "\n",
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "\n",
    "import info.bliki.wiki.dump._\n",
    "import org.xml.sax.SAXException\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Data/Wikipedia-20211016122215.xml\n",
       "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "kvs: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Data/Wikipedia-20211016122215.xml NewHadoopRDD[0] at newAPIHadoopFile at <console>:51\n",
       "rawXmls: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"Data/Wikipedia-20211016122215.xml\"\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
    "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "val rawXmls = kvs.map(_._2.toString).toDS()\n",
    "\n",
    "//경로 지정 후 위키피디아 XML 파일 덤프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: error: Unable to find encoder for type (String, String). An implicit Encoder[(String, String)] is needed to store (String, String) instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.",
     "output_type": "error",
     "traceback": [
      "<console>:42: error: Unable to find encoder for type (String, String). An implicit Encoder[(String, String)] is needed to store (String, String) instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.",
      "       val doctTexts = rawXmls.filter(_ != null).flatMap(wikiXmlToPlainText)",
      "                                                        ^",
      "<console>:16: error: not found: type WikiArticle",
      "       case class Page(var page: WikiArticle = new WikiArticle) {}",
      "                                 ^",
      "<console>:16: error: not found: type WikiArticle",
      "       case class Page(var page: WikiArticle = new WikiArticle) {}",
      "                                                   ^",
      "<console>:18: error: not found: type IArticleFilter",
      "       class ArticleFilter(val Page: Page) extends IArticleFilter{",
      "                                                   ^",
      "<console>:20: error: not found: type WikiArticle",
      "           def process(page: WikiArticle, siteinfo: Siteinfo){",
      "                             ^",
      "<console>:20: error: not found: type Siteinfo",
      "           def process(page: WikiArticle, siteinfo: Siteinfo){",
      "                                                    ^",
      "<console>:19: error: not found: type SAXException",
      "           @throws(classOf[SAXException])",
      "                           ^",
      "<console>:28: error: not found: type WikiXMLParser",
      "               val parser = new WikiXMLParser(new ByteArrayInputStream(pageXml.getBytes), new ArticleFilter(Page))",
      "                                ^",
      "<console>:28: error: not found: type ByteArrayInputStream",
      "               val parser = new WikiXMLParser(new ByteArrayInputStream(pageXml.getBytes), new ArticleFilter(Page))",
      "                                                  ^",
      ""
     ]
    }
   ],
   "source": [
    "//문서의 형식은 날리고 퓨어 텍스트만 남기는 과정\n",
    "\n",
    "case class Page(var page: WikiArticle = new WikiArticle) {}\n",
    "\n",
    "class ArticleFilter(val Page: Page) extends IArticleFilter{\n",
    "    @throws(classOf[SAXException])\n",
    "    def process(page: WikiArticle, siteinfo: Siteinfo){\n",
    "        Page.page = page\n",
    "    }\n",
    "}\n",
    "\n",
    "def wikiXmlToPlainText(pageXml: String): Option[(String, String)] = {\n",
    "    val Page = new Page\n",
    "    try{\n",
    "        val parser = new WikiXMLParser(new ByteArrayInputStream(pageXml.getBytes), new ArticleFilter(Page))\n",
    "        parser.parse()\n",
    "    } catch{\n",
    "        case e: Exception =>\n",
    "    }\n",
    "    val page = Page.page\n",
    "    if (page.getText != null&&page.getTitle != null\n",
    "       && page.getId != null && page.getRevisionId != null\n",
    "       && page.getTimeStamp != null && !page.isTemplate) {\n",
    "        Some((page.getTitle, page.getText))\n",
    "    } else {\n",
    "        None\n",
    "    }\n",
    "}\n",
    "val doctTexts = rawXmls.filter(_ != null).flatMap(wikiXmlToPlainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
